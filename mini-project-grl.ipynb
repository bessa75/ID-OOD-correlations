{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JccJHPDlYvJC"
      },
      "source": [
        "#Importing libraries and GitHub repo cloning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH0R6h5AYkYm"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plRQVpW5RSsX"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Visualization related imports\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Main computation libraries\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "\n",
        "# Deep learning related imports\n",
        "import torch\n",
        "\n",
        "import collections\n",
        "import collections.abc\n",
        "\n",
        "import os\n",
        "import typing\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.datasets as datasets\n",
        "\n",
        "from torch_geometric.nn import GCNConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLstAhZ_pfSn"
      },
      "outputs": [],
      "source": [
        "!pip install munch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BinR99kpjVZ"
      },
      "outputs": [],
      "source": [
        "!pip install ruamel.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWlzLqQ7porE"
      },
      "outputs": [],
      "source": [
        "!pip install tap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZhnNSYpZsZ-"
      },
      "outputs": [],
      "source": [
        "collections.Mapping = collections.abc.Mapping\n",
        "collections.MutableMapping = collections.abc.MutableMapping\n",
        "collections.Callable = collections.abc.Callable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8OILxj-XqSy"
      },
      "source": [
        "#Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5FDNVdRUnV1"
      },
      "outputs": [],
      "source": [
        "from gcn import GCN\n",
        "from gat import GAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ0UJ1H9Zb3J"
      },
      "source": [
        "#Loading GOOD datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ak93o4Ka-F_"
      },
      "outputs": [],
      "source": [
        "from digcopy.dig.oodgraph import GOODMotif,GOODHIV,GOODCBAS,GOODCora,GOODArxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdSv4OXDeBih"
      },
      "outputs": [],
      "source": [
        "data_cora_degree, meta_cora_degree = GOODCora.load('datasets', 'degree', shift='covariate')\n",
        "good_cora_degree=data_cora_degree[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcgKT4YBn5Th"
      },
      "outputs": [],
      "source": [
        "data_cora_word, meta_cora_word = GOODCora.load('datasets', 'word', shift='covariate')\n",
        "good_cora_word=data_cora_word[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYXU2WZ-m0eq"
      },
      "outputs": [],
      "source": [
        "data_arxiv_degree, meta_arxiv_degree = GOODArxiv.load('datasets', 'degree', shift='covariate')\n",
        "good_arxiv_degree=data_arxiv_degree[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGikPBG6oQNW"
      },
      "outputs": [],
      "source": [
        "data_arxiv_time, meta_arxiv_time = GOODArxiv.load('datasets', 'time', shift='covariate')\n",
        "good_arxiv_time=data_arxiv_time[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6DhoWh-ZO_U"
      },
      "outputs": [],
      "source": [
        "data_cbas, meta_cbas = GOODCBAS.load(dataset_root='datasets', domain='color', shift='covariate')\n",
        "good_cbas=data_cbas[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SRPc31DkXC-"
      },
      "source": [
        "#Training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9RQzG9hYgdN"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def train(\n",
        "    params: typing.Dict,good,meta,device,model=None,rate_train_mask=1,earlystop=True,nepochs=None,train_mask=None,optimizer=None,\n",
        ") -> torch.nn.Module:\n",
        "  \"\"\"\n",
        "    This function trains a node classification model and returns the trained model object.\n",
        "    Args :\n",
        "        params (dict): Training hyper-parameters\n",
        "        good (Dataset): Dataset object containing graph data as well as training masks\n",
        "        meta (dict): meta-data\n",
        "        device (PyTorch device)\n",
        "        model (torch.nn.Module): specifies initial model in case of re-training (helpful for our experimental setup)\n",
        "        rate_train_mask (float in [0,1]) : fraction of initial training set used as training data\n",
        "        earlystop (bool) : determines if early stopping will be used to regularize\n",
        "        nepochs (int) : potential respecification of the number of epochs\n",
        "        train_mask (torch.Tensor(bool)) : specification of train_mask to have a consistent fraction of the training set across different models\n",
        "        optimzer (PyTorch optimizer)\n",
        "    Returns :\n",
        "        model (PyTorch model) : trained model\n",
        "        TrainACC (list of float) : history of train accuracies over the epochs\n",
        "        TestACC (list of float) : history of test OOD accuracies\n",
        "        ValIDACC (list of float) : history of validation ID accuracies\n",
        "  \"\"\"\n",
        "\n",
        "  # Set Device\n",
        "  data = good.to(device)\n",
        "\n",
        "  # Reduces the training set if needed by only selecting a fraction\n",
        "  if train_mask is None:\n",
        "    train_mask=torch.zeros(data.train_mask.shape[0],dtype=torch.bool)\n",
        "    if rate_train_mask<1:\n",
        "      for i in range (0,data.train_mask.shape[0]):\n",
        "        if data.train_mask[i] and np.random.uniform()<=rate_train_mask:\n",
        "          train_mask[i]=True\n",
        "    else:\n",
        "      train_mask=copy.deepcopy(data.train_mask)\n",
        "\n",
        "  # Training data\n",
        "  datatrainx=data.x[train_mask]\n",
        "  datatrainy=data.y[train_mask]\n",
        "\n",
        "  # Update parameters\n",
        "  params[\"n_classes\"] = meta.num_classes # number of target classes\n",
        "  print(f\"num classes: {meta.num_classes}\")\n",
        "  params[\"input_dim\"] = meta.dim_node # size of input features\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  # Set Adam optimizer\n",
        "  if optimizer is None:\n",
        "    optimizer=torch.optim.Adam(model.parameters(),lr=params[\"lr\"],weight_decay=params[\"weight_decay\"])\n",
        "\n",
        "  # Set loss\n",
        "  criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "  best_accuracy=0\n",
        "  decreasing_accuracy_count=0\n",
        "  losses=[]\n",
        "  TESTACC=[]\n",
        "  VALIDACC=[]\n",
        "  TRAINACC=[]\n",
        "\n",
        "  # nepochs allows to respecify the number of epochs in case it is not in the provided training_params\n",
        "  if nepochs is None:\n",
        "    nepochs=params[\"epochs\"]\n",
        "\n",
        "  for i in range (0,nepochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward and backward passes\n",
        "    logits=model(data.x,data.edge_index)\n",
        "    loss=criterion(logits[train_mask],datatrainy)\n",
        "    loss.backward()\n",
        "    preds=torch.argmax(logits[train_mask],dim=1)\n",
        "    accuracy=torch.sum(preds==datatrainy)/preds.shape[0]\n",
        "    optimizer.step()\n",
        "\n",
        "    # ID Validation and OOD Test performances\n",
        "    testacc=evaluate(model,data,data.test_mask,printb=False)\n",
        "    validacc=evaluate(model,data,data.val_maskid,printb=False)\n",
        "    TESTACC.append(testacc.item())\n",
        "    VALIDACC.append(validacc.item())\n",
        "    TRAINACC.append(accuracy.item())\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Early stopping\n",
        "    if testacc>best_accuracy:\n",
        "      decreasing_accuracy_count=0\n",
        "    else:\n",
        "      decreasing_accuracy_count+=1\n",
        "    best_accuracy=max(best_accuracy,testacc)\n",
        "    if decreasing_accuracy_count>=params[\"max_patience\"] and earlystop:\n",
        "      print(f\"Early stopping at epoch {i}\")\n",
        "      break\n",
        "\n",
        "    #Displaying performance\n",
        "    if i%params[\"print_time\"]==0:\n",
        "      print(f\"Epoch {i}, Loss : {loss.item()}, Accuracy : {accuracy}\")\n",
        "      evaluate(model,data,data.test_mask,printb=True)\n",
        "\n",
        "  return(model,TRAINACC,TESTACC,VALIDACC,train_mask)\n",
        "\n",
        "  #apply train mask to dataset\n",
        "  #define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bryJJcOVa_YZ"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model,\n",
        "    data,\n",
        "    mask,\n",
        "    printb=True\n",
        "):\n",
        "    \"\"\"\"\n",
        "    This function evaluates the performance of the model on the provided mask\n",
        "    Args :\n",
        "        model (PyTorch model) : model to be evaluated\n",
        "        data (Dataset) : data on which evaluation is done\n",
        "        mask (torch.Tensor(bool)) : mask determining the data subset on which evaluation is done\n",
        "        printb (bool) : determines if the evaluated loss and accuracies are printed for the user\n",
        "    Returns :\n",
        "        accuracy (float) : accuracy of the model\n",
        "    \"\"\"\"\n",
        "\n",
        "    datax=data.x\n",
        "    datay=data.y\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      logits=model(data.x,data.edge_index)\n",
        "      loss=F.cross_entropy(logits[mask],datay[mask])\n",
        "      preds=torch.argmax(logits[mask],dim=1)\n",
        "      accuracy=torch.sum(preds==datay[mask])/preds.shape[0]\n",
        "      if printb:\n",
        "        print(f\"Eval Loss : {loss.item()}, Eval Accuracy :{accuracy}\")\n",
        "      return(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3adS9oMfineZ"
      },
      "source": [
        "#Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nesVfkhKfFEX"
      },
      "outputs": [],
      "source": [
        "def add_validation_maskid(data,rate=0.2):\n",
        "\n",
        "  \"\"\"\n",
        "  Preprocessing of Dataset objects to extract an In-Distribution validation dataset from the training dataset\n",
        "  (typically containing 20% of the training samples)\n",
        "  \"\"\"\n",
        "\n",
        "  val_maskid=torch.zeros(data.train_mask.shape[0],dtype=torch.bool)\n",
        "  for i in range (0,data.train_mask.shape[0]):\n",
        "    if data.train_mask[i] and np.random.uniform()<=rate:\n",
        "      val_maskid[i]=True\n",
        "      data.train_mask[i]=False\n",
        "  data.val_maskid=val_maskid\n",
        "\n",
        "#add_validation_maskid(good_cora_degree)\n",
        "#add_validation_maskid(good_cora_word)\n",
        "add_validation_maskid(good_arxiv_degree)\n",
        "add_validation_maskid(good_arxiv_time)\n",
        "#add_validation_maskid(good_cbas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s2J2SJmXineZ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def retrainings(n,data,meta,model,train_mask,optimizer,device):\n",
        "  \"\"\"\n",
        "  Re-training procedure at the heart of our experimental setup. This functions takes a pre-trained model as input and performs n independent re-trainings over a specified number of epochs, gathers the corresponding accuracies and returns them\n",
        "\n",
        "  Args :\n",
        "      n (int) : number of re-trainings\n",
        "      data (Dataset) : dataset used\n",
        "      meta (dict) : metadata of the dataset used\n",
        "      model (PyTorch model) : pre-trained model regularized with early stopping\n",
        "      train_mask (torch.Tensor(bool)) : mask to specify the training set so that it is consistent with the pre-training\n",
        "      optimizer (PyTorch optimizer) : pre-training optimizer, re-used here to guarantee stable first re-training epochs\n",
        "      device (PyTorch device)\n",
        "\n",
        "  Returns :\n",
        "      AllTrainAcc (list) : Concatenated training accuracies\n",
        "      AllTestOODAcc (list) : Concatenated test OOD accuracies\n",
        "      AllValIDAcc (list) : Concatenated validation ID accuracies\n",
        "  \"\"\"\n",
        "  LTrainAcc,LTestOODAcc,LValIDAcc=[],[],[]\n",
        "\n",
        "\n",
        "  for i in range (0,n):\n",
        "    #Copy pretrained model\n",
        "    model2=copy.deepcopy(model)\n",
        "\n",
        "    #Copy pre-training optimizer\n",
        "    copied_optimizer = type(optimizer)(\n",
        "    model2.parameters(),  # Use parameters from the copied model\n",
        "    **optimizer.defaults  # Copy optimizer settings (e.g., lr, momentum)\n",
        "    )\n",
        "    copied_optimizer.load_state_dict(optimizer.state_dict())\n",
        "\n",
        "    #Re-training of the model\n",
        "    model3,TrainAcc,TestOODAcc,TestIDAcc,train_mask2=train(training_params,data,meta,model=model2,earlystop=False,train_mask=train_mask,optimizer=copied_optimizer,device=device)\n",
        "    LTrainAcc.append(np.array(TrainAcc))\n",
        "    LTestOODAcc.append(np.array(TestOODAcc))\n",
        "    LValIDAcc.append(np.array(TestIDAcc))\n",
        "\n",
        "  #Concatenation of obtained accuracies\n",
        "  AllTrainAcc=np.view(np.array(LTrainAcc),-1)\n",
        "  AllTestOODAcc=np.view(np.array(LTestOODAcc),-1)\n",
        "  AllValIDAcc=np.mean(np.array(LValIDAcc),-1)\n",
        "\n",
        "\n",
        "  return(AllTrainAcc,AllTestOODAcc,AllValIDAcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGoold5Winea"
      },
      "source": [
        "#Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvuksinainec"
      },
      "source": [
        "##Training procedures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTcdeLtXbDPX"
      },
      "outputs": [],
      "source": [
        "training_params = {\n",
        "    \"lr\": 0.004,  # learning rate\n",
        "    \"weight_decay\": 0.0005,  # weight_decay\n",
        "    \"epochs\": 200,  # number of total training epochs\n",
        "    \"max_patience\": 10, # number of k for early stopping\n",
        "    \"hid_dim\": 128, # size of hidden features\n",
        "    \"n_layers\": 2, # number of layers\n",
        "    \"n_heads\": 1,\n",
        "    \"print_time\":20,\n",
        "    \"model_name\": \"GCN\",\n",
        "    \"n_layers_mlp\":2\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "data=good_arxiv_degree\n",
        "meta=meta_arxiv_degree\n",
        "\n",
        "if training_params['model_name'] == 'GCN':\n",
        "    model=GCN(meta[\"dim_node\"],training_params[\"hid_dim\"],training_params[\"n_heads\"],meta[\"num_classes\"],training_params[\"n_layers\"],n_layers_mlp=training_params[\"n_layers_mlp\"]).to(device)\n",
        "elif training_params['model_name'] == 'GAT':\n",
        "    model=GAT(meta[\"dim_node\"],training_params[\"hid_dim\"],training_params[\"n_heads\"],meta[\"num_classes\"],training_params[\"n_layers\"],n_layers_mlp=training_params[\"n_layers_mlp\"]).to(device)\n",
        "elif training_params['model_name'] == 'GATv2':\n",
        "    model=GATv2(meta[\"dim_node\"],training_params[\"hid_dim\"],training_params[\"n_heads\"],meta[\"num_classes\"],training_params[\"n_layers\"]).to(device)\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=training_params[\"lr\"],weight_decay=training_params[\"weight_decay\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYx5Dusx30Qh"
      },
      "outputs": [],
      "source": [
        "#PRETRAINING\n",
        "model,TrainAcc,TestOODAcc,ValIDAcc,train_mask=train(training_params,data,meta,device=device,nepochs=500,rate_train_mask=1,optimizer=optimizer,model=model,earlystop=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rNWxj1K8lut"
      },
      "outputs": [],
      "source": [
        "n_runs=5\n",
        "training_params[\"epochs\"]=16\n",
        "TrainAcc,TestOODAcc,ValIDAcc=retrainings(n_runs,data, meta,model=model,train_mask=train_mask,optimizer=optimizer,device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWh6rMGeLlbQ"
      },
      "source": [
        "##Results Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Lx0mPdOcvCN"
      },
      "outputs": [],
      "source": [
        "#Training curves\n",
        "\n",
        "X=[i for i in range (0,len(TrainAcc))]\n",
        "plt.plot(X,TrainAcc,label=\"Training accuracy\")\n",
        "plt.plot(X,TestOODAcc,label=\"Test OOD accuracy\")\n",
        "plt.plot(X,ValIDAcc,label=\"Val ID accuracy\")\n",
        "plt.plot(X,[max(ValIDAcc)]*len(X),color=\"green\",linestyle=\"dotted\")\n",
        "plt.plot(X,[max(TestOODAcc)]*len(X),color=\"orange\",linestyle=\"dotted\")\n",
        "plt.legend()\n",
        "plt.title(f\"Evolution of losses for {n_runs} averaged runs : CORA Dataset\")\n",
        "plt.xlim(0,500)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tzZwGtD9hac"
      },
      "outputs": [],
      "source": [
        "#Correlation plots\n",
        "\n",
        "plt.plot(TestOODAcc[0:100],ValIDAcc[0:100],\"ob\",color=\"grey\")\n",
        "plt.ylabel(\"Accuracy (%, val-id)\")\n",
        "plt.xlabel(\"Accuracy (%, test-ood)\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr3ANO4Tzznj"
      },
      "outputs": [],
      "source": [
        "print(max(TestOODAcc))\n",
        "print(max(ValIDAcc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1MXPBxsineg"
      },
      "source": [
        "##Indicative - Graph visualization functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR-9-Kido_iE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "def plot_graph(edge_index, node_features=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Graph plotting function\n",
        "\n",
        "    Args :\n",
        "        edge_index : edges of the graph\n",
        "        node_features torch.Tensor (float) : node features of the graph\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    edges = edge_index.t().tolist()\n",
        "    G.add_edges_from(edges)\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    nx.draw(\n",
        "        G,\n",
        "        pos,\n",
        "        with_labels=True,\n",
        "        node_color='lightblue',\n",
        "        edge_color='gray',\n",
        "        node_size=500,\n",
        "        font_size=10\n",
        "    )\n",
        "\n",
        "    if node_features is not None:\n",
        "        for i, (x, y) in enumerate(pos.values()):\n",
        "            plt.text(\n",
        "                x, y + 0.05,\n",
        "                s=f\"{node_features[i]}\",\n",
        "                horizontalalignment='center',\n",
        "                fontsize=8,\n",
        "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=0.5)\n",
        "            )\n",
        "\n",
        "    plt.title(\"Graph Visualization\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_graph(good_cora[0].edge_index)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-TozxRiLgsp"
      },
      "source": [
        "##Indicative - influence of number of parameters (not included in the mini-project report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mz_o4cNinei"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def average_runs(n,data,meta,model,train_mask,optimizer,device):\n",
        "  \"\"\"\n",
        "  Averaging procedure\n",
        "\n",
        "  Args :\n",
        "      n (int) : number of re-trainings\n",
        "      data (Dataset) : dataset used\n",
        "      meta (dict) : metadata of the dataset used\n",
        "      model (PyTorch model) : pre-trained model regularized with early stopping\n",
        "      train_mask (torch.Tensor(bool)) : mask to specify the training set so that it is consistent with the pre-training\n",
        "      optimizer (PyTorch optimizer) : pre-training optimizer, re-used here to guarantee stable first re-training epochs\n",
        "      device (PyTorch device)\n",
        "\n",
        "  Returns :\n",
        "      AvgTrainAcc (list) : Averaged training accuracies\n",
        "      AvgTestOODAcc (list) : Averaged test OOD accuracies\n",
        "      AvgValIDAcc (list) : Averaged validation ID accuracies\n",
        "  \"\"\"\n",
        "\n",
        "  LTrainAcc,LTestOODAcc,LTestIDAcc=[],[],[]\n",
        "  for i in range (0,n):\n",
        "    model2=copy.deepcopy(model)\n",
        "    copied_optimizer = type(optimizer)(\n",
        "    model2.parameters(),\n",
        "    **optimizer.defaults\n",
        ")\n",
        "\n",
        "    copied_optimizer.load_state_dict(optimizer.state_dict())\n",
        "    model3,TrainAcc,TestOODAcc,TestIDAcc,train_mask2=train(training_params,data,meta,model=model2,earlystop=False,train_mask=train_mask,optimizer=copied_optimizer,device=device)\n",
        "    LTrainAcc.append(np.array(TrainAcc))\n",
        "    LTestOODAcc.append(np.array(TestOODAcc))\n",
        "    LTestIDAcc.append(np.array(TestIDAcc))\n",
        "  AvgTrainAcc=np.mean(np.array(LTrainAcc),axis=0)\n",
        "  AvgTestOODAcc=np.mean(np.array(LTestOODAcc),axis=0)\n",
        "  AvgTestIDAcc=np.mean(np.array(LTestIDAcc),axis=0)\n",
        "  return(AvgTrainAcc,AvgTestOODAcc,AvgTestIDAcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQpbMZuQkQIE"
      },
      "outputs": [],
      "source": [
        "n_runs=1\n",
        "hid_dim=[16,32,64,128,256,512]\n",
        "IDTrainDiff=[]\n",
        "OODTrainDiff=[]\n",
        "for h in hid_dim:\n",
        "  training_params[\"hid_dim\"]=h\n",
        "  AvgTrainAcc,AvgTestOODAcc,AvgTestIDAcc=average_runs(n_runs,good_cora_time, meta_cora_time)\n",
        "  IDTrainDiff.append(np.array(AvgTrainAcc)-np.array(AvgTestIDAcc))\n",
        "  OODTrainDiff.append(np.array(AvgTrainAcc)-np.array(AvgTestOODAcc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7D3QbFumXXS"
      },
      "outputs": [],
      "source": [
        "plt.rcParams.update({\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.size\": 12,\n",
        "    \"axes.labelsize\": 14,\n",
        "    \"axes.titlesize\": 14,\n",
        "    \"legend.fontsize\": 12,\n",
        "    \"xtick.labelsize\": 12,\n",
        "    \"ytick.labelsize\": 12,\n",
        "    \"figure.dpi\": 300,\n",
        "    \"lines.linewidth\": 1.5\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zkj3OGB83kWq"
      },
      "outputs": [],
      "source": [
        "X=[i for i in range (0,len(IDTrainDiff[0]))]\n",
        "for i in range (0,len(IDTrainDiff)):\n",
        "  plt.plot(X,IDTrainDiff[i],label=f\"hid_dim : {hid_dim[i]}\")\n",
        "plt.legend()\n",
        "plt.title(f\"CORA Dataset\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Train-Test accuracy [%]\")\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.xlim(0,175)\n",
        "plt.ylim(-0.05,0.29)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Tw5bWW4V_r"
      },
      "outputs": [],
      "source": [
        "X=[i for i in range (0,len(OODTrainDiff[0]))]\n",
        "for i in range (0,len(OODTrainDiff)):\n",
        "  plt.plot(X,OODTrainDiff[i],label=f\"hid_dim : {hid_dim[i]}\")\n",
        "plt.legend()\n",
        "plt.title(f\"CORA Dataset\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Train-Test Accuracy [%]\")\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.xlim(0,175)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}